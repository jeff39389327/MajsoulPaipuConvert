import scrapy
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time
import multiprocessing
from concurrent.futures import ThreadPoolExecutor
import re
import json
from dataclasses import dataclass
from typing import List, Dict
from datetime import datetime, timedelta
import subprocess
import sys
import os
import tempfile

@dataclass
class CrawlerConfig:
    """çˆ¬èŸ²é…ç½®é¡"""
    # çˆ¬èŸ²æ¨¡å¼é¸æ“‡: "auto", "manual", æˆ– "date_room"
    crawler_mode: str = "auto"
    
    # æ‰‹å‹•æ¨¡å¼ï¼šç©å®¶URLsåˆ—è¡¨ (ç•¶ crawler_mode = "manual" æ™‚ä½¿ç”¨)
    manual_player_urls: List[str] = None
    
    # è‡ªå‹•æ¨¡å¼ï¼šæ™‚é–“æ®µè¨­å®š (å¯é¸: "4w", "1w", "3d", "1d")
    time_periods: List[str] = None
    
    # è‡ªå‹•æ¨¡å¼ï¼šæ®µä½è¨­å®š (å¯é¸: "Throne", "Jade", "Gold", "Throne East", "Jade East", "Gold East", "All")
    ranks: List[str] = None
    
    # æ¯å€‹æ™‚é–“æ®µæœ€å¤šæŠ“å–çš„ç©å®¶æ•¸é‡
    max_players_per_period: int = 20
    
    # ç‰Œè­œæ•¸é‡é™åˆ¶åƒæ•¸
    paipu_limit: int = 9999
    
    # date_roomæ¨¡å¼ï¼šæ—¥æœŸå€é–“å’Œç›®æ¨™æˆ¿é–“
    start_date: str = None  # æ ¼å¼: "2019-08-20"
    end_date: str = None    # æ ¼å¼: "2019-08-23"
    target_room: str = None # å¯é¸: "Throne", "Jade", "Gold", "Throne East", "Jade East", "Gold East"
    
    # è¼¸å‡ºæª”æ¡ˆåç¨±
    output_filename: str = "tonpuulist.txt"
    
    # æ˜¯å¦å•Ÿç”¨ç„¡é ­æ¨¡å¼ (headless)
    headless_mode: bool = True
    
    # æ˜¯å¦å„²å­˜é©—è­‰æˆªåœ–
    save_screenshots: bool = True

    @classmethod
    def from_json(cls, json_path: str):
        """å¾JSONæª”æ¡ˆè¼‰å…¥é…ç½®"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return cls(**data)
        except FileNotFoundError:
            print(f"é…ç½®æª”æ¡ˆ {json_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­é…ç½®")
            return cls.get_default_config()
    
    @classmethod
    def get_default_config(cls):
        """å–å¾—é è¨­é…ç½®"""
        return cls(
            crawler_mode="auto",
            manual_player_urls=[],
            time_periods=["4w", "1w", "3d"],
            ranks=["Gold"],
            max_players_per_period=20,
            paipu_limit=9999,
            output_filename="tonpuulist.txt",
            headless_mode=True,
            save_screenshots=True,
            start_date=None,
            end_date=None,
            target_room=None
        )
    
    def save_to_json(self, json_path: str):
        """å„²å­˜é…ç½®åˆ°JSONæª”æ¡ˆ"""
        # è™•ç† None å€¼ï¼Œè½‰æ›ç‚ºç©ºåˆ—è¡¨ä»¥ä¾¿æ–¼JSONåºåˆ—åŒ–
        config_dict = self.__dict__.copy()
        if config_dict.get('manual_player_urls') is None:
            config_dict['manual_player_urls'] = []
        if config_dict.get('time_periods') is None:
            config_dict['time_periods'] = []
        if config_dict.get('ranks') is None:
            config_dict['ranks'] = []
            
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, ensure_ascii=False, indent=2)
    
    def validate(self):
        """é©—è­‰é…ç½®çš„æœ‰æ•ˆæ€§"""
        valid_modes = ["auto", "manual", "date_room"]
        valid_periods = ["4w", "1w", "3d", "1d"]
        valid_ranks = ["Throne", "Jade", "Gold", "Throne East", "Jade East", "Gold East", "All"]
        valid_rooms = ["Throne", "Jade", "Gold", "Throne East", "Jade East", "Gold East"]
        
        # é©—è­‰çˆ¬èŸ²æ¨¡å¼
        if self.crawler_mode not in valid_modes:
            raise ValueError(f"ç„¡æ•ˆçš„çˆ¬èŸ²æ¨¡å¼: {self.crawler_mode}ã€‚æœ‰æ•ˆé¸é …: {valid_modes}")
        
        # æ ¹æ“šæ¨¡å¼é©—è­‰å°æ‡‰åƒæ•¸
        if self.crawler_mode == "manual":
            if not self.manual_player_urls or len(self.manual_player_urls) == 0:
                raise ValueError("æ‰‹å‹•æ¨¡å¼éœ€è¦æä¾› manual_player_urls")
            print(f"âœ… æ‰‹å‹•æ¨¡å¼é…ç½®é©—è­‰é€šé - å·²è¨­å®š {len(self.manual_player_urls)} å€‹ç©å®¶URLs")
            
        elif self.crawler_mode == "auto":
            if not self.time_periods or len(self.time_periods) == 0:
                raise ValueError("è‡ªå‹•æ¨¡å¼éœ€è¦æä¾› time_periods")
            if not self.ranks or len(self.ranks) == 0:
                raise ValueError("è‡ªå‹•æ¨¡å¼éœ€è¦æä¾› ranks")
                
            # é©—è­‰æ™‚é–“æ®µ
            for period in self.time_periods:
                if period not in valid_periods:
                    raise ValueError(f"ç„¡æ•ˆçš„æ™‚é–“æ®µ: {period}ã€‚æœ‰æ•ˆé¸é …: {valid_periods}")
            
            # é©—è­‰æ®µä½
            for rank in self.ranks:
                if rank not in valid_ranks:
                    raise ValueError(f"ç„¡æ•ˆçš„æ®µä½: {rank}ã€‚æœ‰æ•ˆé¸é …: {valid_ranks}")
            
            print(f"âœ… è‡ªå‹•æ¨¡å¼é…ç½®é©—è­‰é€šé")
            
        elif self.crawler_mode == "date_room":
            # é©—è­‰æ—¥æœŸæ ¼å¼å’Œå¿…è¦åƒæ•¸
            if not self.start_date or not self.end_date:
                raise ValueError("date_roomæ¨¡å¼éœ€è¦æä¾› start_date å’Œ end_date")
            if not self.target_room:
                raise ValueError("date_roomæ¨¡å¼éœ€è¦æä¾› target_room")
                
            # é©—è­‰æ—¥æœŸæ ¼å¼
            try:
                start = datetime.strptime(self.start_date, "%Y-%m-%d")
                end = datetime.strptime(self.end_date, "%Y-%m-%d")
                if start > end:
                    raise ValueError("start_date ä¸èƒ½æ™šæ–¼ end_date")
            except ValueError as e:
                raise ValueError(f"æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼ˆæ‡‰ç‚ºYYYY-MM-DDï¼‰: {e}")
            
            # é©—è­‰æˆ¿é–“
            if self.target_room not in valid_rooms:
                raise ValueError(f"ç„¡æ•ˆçš„æˆ¿é–“: {self.target_room}ã€‚æœ‰æ•ˆé¸é …: {valid_rooms}")
            
            print(f"âœ… date_roomæ¨¡å¼é…ç½®é©—è­‰é€šé")
            print(f"  æ—¥æœŸç¯„åœ: {self.start_date} åˆ° {self.end_date}")
            print(f"  ç›®æ¨™æˆ¿é–“: {self.target_room}")
        
        print("âœ… ç¸½é«”é…ç½®é©—è­‰é€šé")

def get_rank_display_name(rank: str) -> Dict[str, str]:
    """å–å¾—æ®µä½çš„é¡¯ç¤ºåç¨±å°æ‡‰"""
    rank_mapping = {
        "Throne": "ç‹åº§",
        "Jade": "ç‰",
        "Gold": "é‡‘",
        "Throne East": "ç‹ä¸œ",
        "Jade East": "ç‰ä¸œ", 
        "Gold East": "é‡‘ä¸œ",
        "All": "å…¨éƒ¨"
    }
    return rank_mapping.get(rank, rank)

def get_period_display_name(period: str) -> str:
    """å–å¾—æ™‚é–“æ®µçš„é¡¯ç¤ºåç¨±"""
    period_mapping = {
        "4w": "å››é€±",
        "1w": "ä¸€é€±", 
        "3d": "ä¸‰å¤©",
        "1d": "ä¸€å¤©"
    }
    return period_mapping.get(period, period)

def execute_date_room_extractor_py(target_date: str, target_room: str, headless_mode: bool = True) -> List[str]:
    """
    åŸ·è¡Œdate_room_extractor.pyä¸¦ç²å–å…¶è¼¸å‡ºçš„ç‰Œè­œIDåˆ—è¡¨
    
    Args:
        target_date: ç›®æ¨™æ—¥æœŸ (æ ¼å¼: "2019-08-23")
        target_room: ç›®æ¨™æˆ¿é–“ (å¦‚: "Throne", "Jade", "Gold" ç­‰)
        headless_mode: æ˜¯å¦ä½¿ç”¨ç„¡é ­æ¨¡å¼
        
    Returns:
        ç‰Œè­œIDåˆ—è¡¨
    """
    # å‰µå»ºè‡¨æ™‚çš„date_room_extractor.pyä¿®æ”¹ç‰ˆæœ¬
    temp_script = """
import sys
sys.path.insert(0, '.')
from date_room_extractor import OptimizedPaipuExtractor, convert_ranks_to_english

def main():
    # åƒæ•¸è¨­å®š
    target_date = "{target_date}"
    target_ranks = ["{target_room}"]
    max_paipus = 99999
    headless_mode = {headless_mode}
    
    target_ranks = convert_ranks_to_english(target_ranks)
    
    extractor = OptimizedPaipuExtractor(headless=headless_mode)
    
    try:
        results = extractor.extract_from_rooms(
            target_date=target_date,
            target_ranks=target_ranks,
            max_paipus=max_paipus
        )
        
        # åªè¼¸å‡ºç‰Œè­œIDï¼Œæ¯è¡Œä¸€å€‹
        for paipu in results:
            print(paipu)
        
    finally:
        extractor.close()

if __name__ == "__main__":
    main()
""".format(
        target_date=target_date,
        target_room=target_room,
        headless_mode=str(headless_mode)
    )
    
    # å‰µå»ºè‡¨æ™‚æª”æ¡ˆ
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as temp_file:
        temp_file.write(temp_script)
        temp_file_path = temp_file.name
    
    try:
        # åŸ·è¡Œè‡¨æ™‚è…³æœ¬
        result = subprocess.run(
            [sys.executable, temp_file_path],
            capture_output=True,
            text=True,
            encoding='utf-8'
        )
        
        if result.returncode != 0:
            print(f"åŸ·è¡Œdate_room_extractor.pyæ™‚å‡ºéŒ¯: {result.stderr}")
            return []
        
        # è§£æè¼¸å‡ºï¼Œæ¯è¡Œä¸€å€‹ç‰Œè­œID
        paipu_ids = []
        for line in result.stdout.strip().split('\n'):
            line = line.strip()
            # éæ¿¾æ‰éç‰Œè­œIDçš„è¼¸å‡ºï¼ˆå¦‚printçš„èª¿è©¦ä¿¡æ¯ï¼‰
            if line and re.match(r'^[0-9]{6}-[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', line):
                paipu_ids.append(line)
        
        return paipu_ids
        
    finally:
        # åˆªé™¤è‡¨æ™‚æª”æ¡ˆ
        try:
            os.unlink(temp_file_path)
        except:
            pass

def collect_paipus_by_date_room(config: CrawlerConfig) -> List[str]:
    """ä½¿ç”¨date_roomæ¨¡å¼æ”¶é›†ç‰Œè­œ"""
    all_paipus = []
    
    try:
        # è§£ææ—¥æœŸç¯„åœ
        start_date = datetime.strptime(config.start_date, "%Y-%m-%d")
        end_date = datetime.strptime(config.end_date, "%Y-%m-%d")
        
        # è¨ˆç®—ç¸½å¤©æ•¸
        total_days = (end_date - start_date).days + 1
        print(f"\n=== é–‹å§‹ date_room æ¨¡å¼æ”¶é›† ===")
        print(f"æ—¥æœŸç¯„åœ: {config.start_date} åˆ° {config.end_date} (å…± {total_days} å¤©)")
        print(f"ç›®æ¨™æˆ¿é–“: {config.target_room}")
        print(f"ç„¡é ­æ¨¡å¼: {config.headless_mode}")
        print("="*50)
        
        # è™•ç†æ¯ä¸€å¤©
        current_date = start_date
        day_count = 0
        
        while current_date <= end_date:
            day_count += 1
            date_str = current_date.strftime("%Y-%m-%d")
            print(f"\n[{day_count}/{total_days}] æ­£åœ¨è™•ç†æ—¥æœŸ: {date_str}")
            
            # åŸ·è¡Œdate_room_extractor.pyç²å–ç•¶å¤©çš„ç‰Œè­œ
            day_results = execute_date_room_extractor_py(
                target_date=date_str,
                target_room=config.target_room,
                headless_mode=config.headless_mode
            )
            
            # æ·»åŠ åˆ°ç¸½åˆ—è¡¨ï¼ˆdate_room_extractor.pyå·²ç¶“å»é‡ï¼Œä½†é€™è£¡å†æ¬¡ç¢ºä¿è·¨æ—¥æœŸçš„å»é‡ï¼‰
            for paipu in day_results:
                if paipu not in all_paipus:
                    all_paipus.append(paipu)
            
            print(f"  âœ“ {date_str} æ”¶é›†åˆ° {len(day_results)} å€‹ç‰Œè­œ")
            print(f"  ç´¯è¨ˆæ”¶é›†: {len(all_paipus)} å€‹ä¸é‡è¤‡ç‰Œè­œ")
            
            # ç§»åˆ°ä¸‹ä¸€å¤©
            current_date += timedelta(days=1)
            
            # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å¤©ï¼Œç¨å¾®ç­‰å¾…ä¸€ä¸‹
            if current_date <= end_date:
                time.sleep(1)
        
        print(f"\n=== date_room æ¨¡å¼æ”¶é›†å®Œæˆ ===")
        print(f"ç¸½è¨ˆæ”¶é›†åˆ° {len(all_paipus)} å€‹ä¸é‡è¤‡çš„ç‰Œè­œID")
        
    except Exception as e:
        print(f"date_roomæ¨¡å¼åŸ·è¡Œå‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()
    
    return all_paipus

def setup_rank_selection(driver, target_ranks: List[str]):
    """è¨­å®šæ®µä½é¸æ“‡"""
    all_available_ranks = ["Throne", "Jade", "Gold", "Throne East", "Jade East", "Gold East"]
    
    try:
        print("æ­£åœ¨é…ç½®æ®µä½é¸æ“‡...")
        
        # å¦‚æœé¸æ“‡"å…¨éƒ¨"ï¼Œç›´æ¥ä½¿ç”¨ç¶²é é è¨­ç‹€æ…‹ï¼ˆæ‰€æœ‰æ®µä½éƒ½å·²é¸ä¸­ï¼‰
        if "All" in target_ranks:
            print("é¸æ“‡å…¨éƒ¨æ®µä½ - ä½¿ç”¨ç¶²é é è¨­ç‹€æ…‹ï¼Œç„¡éœ€é»æ“Š")
            print("ç¶²é é è¨­å·²é¸ä¸­æ‰€æœ‰æ®µä½ï¼Œè·³éæ®µä½é¸æ“‡æ“ä½œ")
            return
        
        # å…ˆå–æ¶ˆé¸æ“‡æ‰€æœ‰æ®µä½
        for rank in all_available_ranks:
            try:
                # å˜—è©¦è‹±æ–‡æ¨™ç±¤
                rank_label = driver.find_element(By.XPATH, f"//span[contains(@class, 'MuiFormControlLabel-label') and text()='{rank}']")
                checkbox = rank_label.find_element(By.XPATH, "./preceding-sibling::span//input[@type='checkbox']")
                
                if checkbox.is_selected():
                    print(f"å–æ¶ˆé¸æ“‡æ®µä½: {rank}")
                    driver.execute_script("arguments[0].click();", rank_label)
                    time.sleep(0.5)
            except:
                # å˜—è©¦ä¸­æ–‡æ¨™ç±¤
                try:
                    chinese_rank = get_rank_display_name(rank)
                    rank_label = driver.find_element(By.XPATH, f"//span[contains(@class, 'MuiFormControlLabel-label') and text()='{chinese_rank}']")
                    checkbox = rank_label.find_element(By.XPATH, "./preceding-sibling::span//input[@type='checkbox']")
                    
                    if checkbox.is_selected():
                        print(f"å–æ¶ˆé¸æ“‡æ®µä½: {chinese_rank}")
                        driver.execute_script("arguments[0].click();", rank_label)
                        time.sleep(0.5)
                except:
                    continue
        
        # é¸æ“‡ç›®æ¨™æ®µä½
        for rank in target_ranks:
            try:
                # å˜—è©¦è‹±æ–‡æ¨™ç±¤
                rank_label = driver.find_element(By.XPATH, f"//span[contains(@class, 'MuiFormControlLabel-label') and text()='{rank}']")
                checkbox = rank_label.find_element(By.XPATH, "./preceding-sibling::span//input[@type='checkbox']")
                
                if not checkbox.is_selected():
                    print(f"é¸æ“‡æ®µä½: {rank}")
                    driver.execute_script("arguments[0].click();", rank_label)
                    time.sleep(0.5)
                else:
                    print(f"æ®µä½ {rank} å·²é¸ä¸­")
            except:
                # å˜—è©¦ä¸­æ–‡æ¨™ç±¤
                try:
                    chinese_rank = get_rank_display_name(rank)
                    rank_label = driver.find_element(By.XPATH, f"//span[contains(@class, 'MuiFormControlLabel-label') and text()='{chinese_rank}']")
                    checkbox = rank_label.find_element(By.XPATH, "./preceding-sibling::span//input[@type='checkbox']")
                    
                    if not checkbox.is_selected():
                        print(f"é¸æ“‡æ®µä½: {chinese_rank}")
                        driver.execute_script("arguments[0].click();", rank_label)
                        time.sleep(0.5)
                    else:
                        print(f"æ®µä½ {chinese_rank} å·²é¸ä¸­")
                except Exception as e:
                    print(f"ç„¡æ³•é¸æ“‡æ®µä½ {rank}: {e}")
        
        # ç­‰å¾…é é¢æ›´æ–°
        time.sleep(3)
        print("æ®µä½é¸æ“‡é…ç½®å®Œæˆ")
        
    except Exception as e:
        print(f"é…ç½®æ®µä½é¸æ“‡æ™‚å‡ºéŒ¯: {e}")

def get_top_players_urls(config: CrawlerConfig):
    """æ ¹æ“šé…ç½®è‡ªå‹•æŠ“å–æ’è¡Œæ¦œç©å®¶çš„URLs"""
    chrome_options = Options()
    if config.headless_mode:
        chrome_options.add_argument("--headless")
    
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-gpu")
    driver = webdriver.Chrome(options=chrome_options)
    
    all_player_urls = []
    
    try:
        # å­˜å–æ’åé é¢
        driver.get("https://amae-koromo.sapk.ch/ranking/delta")
        
        # ç­‰å¾…é é¢è¼‰å…¥å®Œæˆ
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        time.sleep(5)
        
        # å»ºç«‹æ®µä½é¡¯ç¤ºå­—ä¸²
        rank_display = ", ".join([get_rank_display_name(rank) for rank in config.ranks])
        period_display = ", ".join([get_period_display_name(period) for period in config.time_periods])
        
        print(f"æ­£åœ¨æŠ“å–æ±ªæ±ªæ¦œæ’å")
        print(f"ç›®æ¨™æ™‚é–“æ®µ: {period_display}")
        print(f"ç›®æ¨™æ®µä½: {rank_display}")
        
        # è¨­å®šæ®µä½é¸æ“‡
        setup_rank_selection(driver, config.ranks)
        
        # å„²å­˜æ®µä½é¸æ“‡é©—è­‰æˆªåœ–
        if config.save_screenshots:
            driver.save_screenshot("screenshot_rank_selection_verification.png")
            print("å·²å„²å­˜æ®µä½é¸æ“‡é©—è­‰æˆªåœ–: screenshot_rank_selection_verification.png")
        
        # è™•ç†æ¯å€‹æ™‚é–“æ®µ
        for period in config.time_periods:
            print(f"\n=== é–‹å§‹è™•ç†æ™‚é–“æ®µ: {get_period_display_name(period)} ({period}) ===")
            
            try:
                # æŸ¥æ‰¾ä¸¦é»æ“Šå°æ‡‰çš„æ™‚é–“æ®µradioæŒ‰éˆ•
                print(f"æŸ¥æ‰¾æ™‚é–“æ®µ {period} çš„radioæŒ‰éˆ•...")
                
                radio_button = driver.find_element(By.CSS_SELECTOR, f'input[type="radio"][value="{period}"]')
                print(f"æ‰¾åˆ° {period} çš„radioæŒ‰éˆ•")
                
                # é»æ“ŠradioæŒ‰éˆ•
                driver.execute_script("arguments[0].click();", radio_button)
                print(f"å·²é»æ“Š {period} æ™‚é–“æ®µ")
                
                # ç­‰å¾…é é¢æ›´æ–°
                time.sleep(5)
                
                # å„²å­˜é©—è­‰æˆªåœ–
                if config.save_screenshots:
                    rank_suffix = "_".join(config.ranks).lower()
                    screenshot_filename = f"screenshot_{period}_positive_ranking_{rank_suffix}.png"
                    driver.save_screenshot(screenshot_filename)
                    print(f"å·²å„²å­˜æˆªåœ–: {screenshot_filename}")
                
            except Exception as e:
                print(f"åˆ‡æ›åˆ°æ™‚é–“æ®µ {period} æ™‚å‡ºéŒ¯: {e}")
                continue
            
            # å–å¾—è©²æ™‚é–“æ®µçš„ç©å®¶é€£çµ
            period_player_urls = extract_positive_ranking_players(driver, period, config)
            all_player_urls.extend(period_player_urls)
            
            print(f"æ™‚é–“æ®µ {period} å–å¾—åˆ° {len(period_player_urls)} å€‹ç©å®¶URL")
        
        # å»é‡è™•ç†
        unique_player_urls = []
        seen_players = set()
        
        for url in all_player_urls:
            player_id_match = re.search(r'/player/(\d+)', url)
            if player_id_match:
                player_id = player_id_match.group(1)
                if player_id not in seen_players:
                    seen_players.add(player_id)
                    unique_player_urls.append(url)
        
        print(f"å…±å–å¾—åˆ° {len(unique_player_urls)} å€‹ä¸é‡è¤‡çš„ç©å®¶URLsï¼ˆ/12æ¨¡å¼ï¼‰")
        
        if config.save_screenshots:
            print(f"\nğŸ“¸ é©—è­‰æˆªåœ–å·²å„²å­˜:")
            print(f"  - screenshot_rank_selection_verification.png (æ®µä½é¸æ“‡é©—è­‰)")
            for period in config.time_periods:
                rank_suffix = "_".join(config.ranks).lower()
                print(f"  - screenshot_{period}_positive_ranking_{rank_suffix}.png ({get_period_display_name(period)})")
        
        return unique_player_urls
        
    except Exception as e:
        print(f"æŠ“å–æ’åæ™‚å‡ºéŒ¯: {e}")
        return []
    
    finally:
        driver.quit()

def extract_positive_ranking_players(driver, period, config: CrawlerConfig):
    """å¾Positive rankingåˆ—ä¸­æå–ç©å®¶é€£çµ"""
    player_urls = []
    
    try:
        print(f"é–‹å§‹æŸ¥æ‰¾æ™‚é–“æ®µ {period} çš„Positive rankingåˆ—ä¸­çš„ç©å®¶é€£çµ...")
        
        # æ–¹æ³•1ï¼šå˜—è©¦æŸ¥æ‰¾Positive rankingåˆ—ä¸­çš„ç©å®¶é€£çµ
        player_links_in_positive = []
        
        try:
            positive_heading = driver.find_element(By.XPATH, "//*[contains(text(), 'Positive ranking')]")
            print("æ‰¾åˆ°Positive rankingæ¨™é¡Œ")
            
            positive_container = positive_heading.find_element(By.XPATH, "./following-sibling::*[1] | ./parent::*/following-sibling::*[1]")
            
            container_links = positive_container.find_elements(By.CSS_SELECTOR, "a[href*='/player/']")
            player_links_in_positive.extend(container_links)
            print(f"åœ¨Positive rankingå®¹å™¨ä¸­æ‰¾åˆ° {len(container_links)} å€‹ç©å®¶é€£çµ")
            
        except Exception as e:
            print(f"æ–¹æ³•1å¤±æ•—: {e}")
        
        # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±æ•—ï¼Œå˜—è©¦é€šéé é¢ä½ˆå±€å®šä½
        if not player_links_in_positive:
            try:
                print("å˜—è©¦æ–¹æ³•2ï¼šé€šéé é¢ä½ˆå±€å®šä½Positive ranking...")
                
                all_player_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='/player/']")
                
                for link in all_player_links:
                    try:
                        location = link.location
                        size = driver.get_window_size()
                        
                        if size['width'] * 0.33 < location['x'] < size['width'] * 0.66:
                            player_links_in_positive.append(link)
                    except:
                        continue
                
                print(f"æ–¹æ³•2æ‰¾åˆ° {len(player_links_in_positive)} å€‹å¯èƒ½çš„Positive rankingé€£çµ")
                
            except Exception as e:
                print(f"æ–¹æ³•2ä¹Ÿå¤±æ•—: {e}")
        
        # æ–¹æ³•3ï¼šå¦‚æœå‰å…©ç¨®æ–¹æ³•éƒ½å¤±æ•—ï¼Œå–å¾—æ‰€æœ‰ç©å®¶é€£çµä¸¦éæ¿¾
        if not player_links_in_positive:
            print("å˜—è©¦æ–¹æ³•3ï¼šå–å¾—æ‰€æœ‰ç©å®¶é€£çµ...")
            all_links = driver.find_elements(By.TAG_NAME, "a")
            for link in all_links:
                href = link.get_attribute("href")
                if href and "/player/" in href:
                    player_links_in_positive.append(link)
            
            print(f"æ–¹æ³•3æ‰¾åˆ° {len(player_links_in_positive)} å€‹ç©å®¶é€£çµ")
            if len(player_links_in_positive) >= 60:
                start_idx = len(player_links_in_positive) // 3
                end_idx = start_idx + config.max_players_per_period
                player_links_in_positive = player_links_in_positive[start_idx:end_idx]
                print(f"éæ¿¾å¾Œä¿ç•™ {len(player_links_in_positive)} å€‹Positive rankingé€£çµ")
        
        # æå–æŒ‡å®šæ•¸é‡çš„ä¸é‡è¤‡ç©å®¶URL
        seen_players = set()
        for link in player_links_in_positive:
            href = link.get_attribute("href") if hasattr(link, 'get_attribute') else getattr(link, 'href', None)
            if href and "/player/" in href:
                player_id_match = re.search(r'/player/(\d+)', href)
                if player_id_match:
                    player_id = player_id_match.group(1)
                    if player_id not in seen_players:
                        seen_players.add(player_id)
                        base_url = f"https://amae-koromo.sapk.ch/player/{player_id}/12"
                        url = f"{base_url}?limit={config.paipu_limit}"
                        player_urls.append(url)
                        print(f"æ·»åŠ ç©å®¶URL ({period}): {url}")
                        
                        if len(player_urls) >= config.max_players_per_period:
                            break
        
    except Exception as e:
        print(f"æå–æ™‚é–“æ®µ {period} çš„ç©å®¶é€£çµæ™‚å‡ºéŒ¯: {e}")
    
    return player_urls

def process_player(url, processed_paipu_ids, player_counts, config: CrawlerConfig):
    """è™•ç†å–®å€‹ç©å®¶çš„ç‰Œè­œæŠ“å–"""
    chrome_options = Options()
    if config.headless_mode:
        chrome_options.add_argument("--headless")
    driver = webdriver.Chrome(options=chrome_options)
    
    try:
        driver.get(url)

        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        time.sleep(2)

        while True:
            paipu_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='paipu=']")
            
            if not paipu_links:
                paipu_links = driver.find_elements(By.XPATH, "//a[contains(@href, 'paipu=')]")

            new_paipu_found = False
            for link in paipu_links:
                href = link.get_attribute("href")
                if href and "paipu=" in href:
                    paipu_id = href.split("paipu=")[1].split("_")[0]
                    
                    if paipu_id not in processed_paipu_ids:
                        processed_paipu_ids.append(paipu_id)
                        player_counts[url] += 1
                        print(f"å·²å¯«å…¥æ–°çš„ç‰Œè­œ ({url}):", paipu_id)
                        new_paipu_found = True
            
            driver.execute_script("window.scrollBy(0, 500);")
            time.sleep(0.3)

            if driver.execute_script("return window.innerHeight + window.scrollY + 10 >= document.body.offsetHeight"):
                break
                
            if not new_paipu_found:
                time.sleep(1)

        print(f"ç©å®¶ {url} æ”¶é›†åˆ° {player_counts[url]} å€‹ç‰Œè­œID")
        
    except Exception as e:
        print(f"è™•ç†ç©å®¶ {url} æ™‚å‡ºéŒ¯: {e}")
    finally:
        driver.quit()

class PaipuSpider(scrapy.Spider):
    name = "paipu_spider"

    def __init__(self, config_path: str = "crawler_config.json"):
        # è¼‰å…¥é…ç½®
        self.config = CrawlerConfig.from_json(config_path)
        self.config.validate()
        
        self.manager = multiprocessing.Manager()
        self.processed_paipu_ids = self.manager.list()
        
        # æ ¹æ“šé…ç½®æ¨¡å¼æ±ºå®šä½¿ç”¨æ–¹å¼
        if self.config.crawler_mode == "manual":
            print("ğŸ”§ ä½¿ç”¨ Manual æ¨¡å¼ï¼ˆLegacyç›¸å®¹ï¼‰...")
            print(f"å¾é…ç½®æª”æ¡ˆä¸­è®€å– {len(self.config.manual_player_urls)} å€‹æ‰‹å‹•è¨­å®šçš„ç©å®¶URLs")
            
            # ä½¿ç”¨é…ç½®æª”æ¡ˆä¸­çš„æ‰‹å‹•URLs
            self.player_urls = []
            for url in self.config.manual_player_urls:
                # ç¢ºä¿URLæ ¼å¼æ­£ç¢ºï¼Œæ·»åŠ limitåƒæ•¸
                if "/player/" in url and "?limit=" not in url:
                    url = f"{url}?limit={self.config.paipu_limit}"
                elif "/player/" in url and "?limit=" in url:
                    # URLå·²ç¶“æœ‰limitåƒæ•¸ï¼Œä½¿ç”¨åŸå§‹URL
                    pass
                else:
                    print(f"âš ï¸  è·³éç„¡æ•ˆçš„URLæ ¼å¼: {url}")
                    continue
                self.player_urls.append(url)
            
            print(f"å·²è¼‰å…¥ {len(self.player_urls)} å€‹æœ‰æ•ˆçš„ç©å®¶URLs")
            self.player_counts = self.manager.dict({url: 0 for url in self.player_urls})
            
        elif self.config.crawler_mode == "date_room":
            print("ğŸ“… ä½¿ç”¨ date_room æ¨¡å¼...")
            # date_roomæ¨¡å¼ä¸éœ€è¦player_urls
            self.player_urls = []
            self.player_counts = self.manager.dict()
            
        else:  # auto mode
            print("ğŸš€ ä½¿ç”¨è‡ªå‹•åŒ–é…ç½®æ¨¡å¼...")
            print(f"é…ç½®æ‘˜è¦:")
            print(f"  æ™‚é–“æ®µ: {[get_period_display_name(p) for p in self.config.time_periods]}")
            print(f"  æ®µä½: {[get_rank_display_name(r) for r in self.config.ranks]}")
            print(f"  æ¯å€‹æ™‚é–“æ®µæœ€å¤šç©å®¶æ•¸: {self.config.max_players_per_period}")
            print(f"  ç‰Œè­œé™åˆ¶: {self.config.paipu_limit}")
            
            self.player_urls = get_top_players_urls(self.config)
            self.player_counts = self.manager.dict({url: 0 for url in self.player_urls})

        # è®€å–å·²æœ‰çš„ç‰Œè­œIDï¼ˆæ‰€æœ‰æ¨¡å¼éƒ½éœ€è¦ï¼‰
        try:
            with open(self.config.output_filename, "r") as file:
                for line in file:
                    paipu_id = line.strip()
                    if paipu_id:
                        self.processed_paipu_ids.append(paipu_id)
            print(f"å·²è¼‰å…¥ {len(self.processed_paipu_ids)} å€‹å·²è™•ç†çš„ç‰Œè­œID")
        except FileNotFoundError:
            print(f"æœªæ‰¾åˆ°{self.config.output_filename}æª”æ¡ˆï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆ")

    def start_requests(self):
        yield scrapy.Request(url="https://amae-koromo.sapk.ch", callback=self.start_crawling)

    def start_crawling(self, response):
        if self.config.crawler_mode == "date_room":
            # date_roomæ¨¡å¼ï¼šç›´æ¥èª¿ç”¨æ”¶é›†å‡½æ•¸
            date_room_paipus = collect_paipus_by_date_room(self.config)
            
            # æ·»åŠ åˆ°processed_paipu_idsä¸­ï¼ˆé¿å…é‡è¤‡ï¼‰
            for paipu_id in date_room_paipus:
                if paipu_id not in self.processed_paipu_ids:
                    self.processed_paipu_ids.append(paipu_id)
            
            # ç›´æ¥çµæŸ
            self.spider_closed(None)
            
        else:
            # åŸæœ‰çš„autoå’Œmanualæ¨¡å¼è™•ç†
            print(f"é–‹å§‹è™•ç† {len(self.player_urls)} å€‹ç©å®¶...")
            
            processes = []
            for url in self.player_urls:
                process = multiprocessing.Process(target=process_player, args=(url, self.processed_paipu_ids, self.player_counts, self.config))
                processes.append(process)
                process.start()

            for process in processes:
                process.join()

            self.spider_closed(None)

    def spider_closed(self, reason):
        print(f"å…±æ”¶é›†åˆ° {len(self.processed_paipu_ids)} å€‹ä¸é‡è¤‡çš„ç‰Œè­œID")
        
        if self.config.crawler_mode == "date_room":
            print("\nğŸ“‹ date_roomæ¨¡å¼é…ç½®æ‘˜è¦:")
            print(f"  æ—¥æœŸç¯„åœ: {self.config.start_date} åˆ° {self.config.end_date}")
            print(f"  ç›®æ¨™æˆ¿é–“: {self.config.target_room}")
        else:
            print("å„ç©å®¶æ”¶é›†åˆ°çš„ç‰Œè­œIDæ•¸é‡:")
            
            total_paipu = 0
            for url in self.player_urls:
                count = self.player_counts[url]
                total_paipu += count
                print(f"{url}: {count}")
            
            print(f"\nç¸½è¨ˆæ”¶é›†ç‰Œè­œæ•¸é‡: {total_paipu}")
            
            # é¡¯ç¤ºé…ç½®æ‘˜è¦
            if self.config.crawler_mode == "auto":
                print(f"\nğŸ“‹ ä½¿ç”¨çš„é…ç½®:")
                print(f"  æ™‚é–“æ®µ: {', '.join([get_period_display_name(p) for p in self.config.time_periods])}")
                print(f"  æ®µä½: {', '.join([get_rank_display_name(r) for r in self.config.ranks])}")
        
        if self.config.save_screenshots and self.config.crawler_mode == "auto":
            print(f"\nğŸ“¸ é©—è­‰æˆªåœ–å·²å„²å­˜:")
            print(f"  - screenshot_rank_selection_verification.png (æ®µä½é¸æ“‡é©—è­‰)")
            for period in self.config.time_periods:
                rank_suffix = "_".join(self.config.ranks).lower()
                print(f"  - screenshot_{period}_positive_ranking_{rank_suffix}.png ({get_period_display_name(period)})")

        with ThreadPoolExecutor() as executor:
            executor.submit(self.write_to_file)

    def write_to_file(self):
        with open(self.config.output_filename, "w") as file:
            for paipu_id in self.processed_paipu_ids:
                file.write(paipu_id + "\n")
        print(f"ç‰Œè­œIDå·²å„²å­˜åˆ° {self.config.output_filename}")

# å»ºç«‹é è¨­é…ç½®æª”æ¡ˆçš„å‡½æ•¸
def create_default_config():
    """å»ºç«‹é è¨­é…ç½®æª”æ¡ˆ"""
    config = CrawlerConfig.get_default_config()
    config.save_to_json("crawler_config.json")
    print("å·²å»ºç«‹é è¨­é…ç½®æª”æ¡ˆ: crawler_config.json")
    return config

# å»ºç«‹date_roomæ¨¡å¼çš„ç¯„ä¾‹é…ç½®
def create_date_room_config_example():
    """å»ºç«‹date_roomæ¨¡å¼çš„ç¯„ä¾‹é…ç½®æª”æ¡ˆ"""
    config = CrawlerConfig(
        crawler_mode="date_room",
        start_date="2019-08-20",
        end_date="2019-08-23",
        target_room="Jade",
        output_filename="date_room_list.txt",
        headless_mode=True,
        save_screenshots=False
    )
    config.save_to_json("date_room_config_example.json")
    print("å·²å»ºç«‹date_roomæ¨¡å¼ç¯„ä¾‹é…ç½®æª”æ¡ˆ: date_room_config_example.json")
    return config

# ==========================================
# ä½¿ç”¨èªªæ˜å’ŒåŸ·è¡Œæ–¹å¼
# ==========================================

if __name__ == "__main__":
    # æ–¹å¼1ï¼šè‡ªå‹•åŒ–é…ç½®æ¨¡å¼ï¼ˆæ¨è–¦ï¼‰
    # åœ¨ crawler_config.json ä¸­è¨­å®šï¼š
    # {
    #   "crawler_mode": "auto",
    #   "time_periods": ["4w", "1w", "3d"],
    #   "ranks": ["Gold"],
    #   ...
    # }
    # åŸ·è¡Œå‘½ä»¤ï¼šscrapy crawl paipu_spider
    
    # æ–¹å¼2ï¼šæ‰‹å‹•æ¨¡å¼ï¼ˆLegacy Manual ç›¸å®¹ï¼‰
    # åœ¨ crawler_config.json ä¸­è¨­å®šï¼š
    # {
    #   "crawler_mode": "manual",
    #   "manual_player_urls": [
    #     "https://amae-koromo.sapk.ch/player/123456/12",
    #     "https://amae-koromo.sapk.ch/player/789012/12"
    #   ],
    #   ...
    # }
    # åŸ·è¡Œå‘½ä»¤ï¼šscrapy crawl paipu_spider
    
    # æ–¹å¼3ï¼šdate_roomæ¨¡å¼ï¼ˆæ–°å¢ï¼‰
    # åœ¨ crawler_config.json ä¸­è¨­å®šï¼š
    # {
    #   "crawler_mode": "date_room",
    #   "start_date": "2019-08-20",
    #   "end_date": "2019-08-23",
    #   "target_room": "Jade",
    #   "output_filename": "list.txt",
    #   "headless_mode": true,
    #   "save_screenshots": true
    # }
    # åŸ·è¡Œå‘½ä»¤ï¼šscrapy crawl paipu_spider
    
    # å¦‚æœé…ç½®æª”æ¡ˆä¸å­˜åœ¨ï¼Œå»ºç«‹é è¨­é…ç½®
    import os
    if not os.path.exists("crawler_config.json"):
        create_default_config()
        print("å·²å»ºç«‹é è¨­é…ç½®æª”æ¡ˆ: crawler_config.json")
        print("è«‹ç·¨è¼¯é…ç½®æª”æ¡ˆä¾†è‡ªè¨‚æ‚¨çš„æŠ“å–è¨­å®š")
        print("\nğŸ“‹ å¯ç”¨çš„é…ç½®æ¨¡å¼:")
        print("  - crawler_mode: 'auto' (è‡ªå‹•åŒ–)")
        print("  - crawler_mode: 'manual' (æ‰‹å‹•)")
        print("  - crawler_mode: 'date_room' (æ—¥æœŸæˆ¿é–“æ¨¡å¼)")
        print("  - è©³ç´°è¨­å®šè«‹åƒè€ƒé…ç½®æª”æ¡ˆä¸­çš„ç¯„ä¾‹")
        
        # åŒæ™‚å»ºç«‹date_roomæ¨¡å¼çš„ç¯„ä¾‹
        if not os.path.exists("date_room_config_example.json"):
            create_date_room_config_example()
    else:
        print("ç™¼ç¾ç¾æœ‰é…ç½®æª”æ¡ˆ: crawler_config.json")
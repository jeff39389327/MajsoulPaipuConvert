import scrapy
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time
import multiprocessing
from concurrent.futures import ThreadPoolExecutor
import re
import json
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class CrawlerConfig:
    """çˆ¬èŸ²é…ç½®é¡"""
    # æ™‚é–“æ®µè¨­å®š (å¯é¸: "4w", "1w", "3d", "1d")
    time_periods: List[str]
    
    # æ®µä½è¨­å®š (å¯é¸: "Throne", "Jade", "Gold", "Throne East", "Jade East", "Gold East", "All")
    ranks: List[str]
    
    # æ¯å€‹æ™‚é–“æ®µæœ€å¤šæŠ“å–çš„ç©å®¶æ•¸é‡
    max_players_per_period: int = 20
    
    # ç‰Œè­œæ•¸é‡é™åˆ¶åƒæ•¸
    paipu_limit: int = 9999
    
    # è¼¸å‡ºæª”æ¡ˆåç¨±
    output_filename: str = "tonpuulist.txt"
    
    # æ˜¯å¦å•Ÿç”¨ç„¡é ­æ¨¡å¼ (headless)
    headless_mode: bool = True
    
    # æ˜¯å¦å„²å­˜é©—è­‰æˆªåœ–
    save_screenshots: bool = True

    @classmethod
    def from_json(cls, json_path: str):
        """å¾JSONæª”æ¡ˆè¼‰å…¥é…ç½®"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return cls(**data)
        except FileNotFoundError:
            print(f"é…ç½®æª”æ¡ˆ {json_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­é…ç½®")
            return cls.get_default_config()
    
    @classmethod
    def get_default_config(cls):
        """å–å¾—é è¨­é…ç½®"""
        return cls(
            time_periods=["4w", "1w", "3d"],
            ranks=["Gold"],
            max_players_per_period=20,
            paipu_limit=9999,
            output_filename="tonpuulist.txt",
            headless_mode=True,
            save_screenshots=True
        )
    
    def save_to_json(self, json_path: str):
        """å„²å­˜é…ç½®åˆ°JSONæª”æ¡ˆ"""
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.__dict__, f, ensure_ascii=False, indent=2)
    
    def validate(self):
        """é©—è­‰é…ç½®çš„æœ‰æ•ˆæ€§"""
        valid_periods = ["4w", "1w", "3d", "1d"]
        valid_ranks = ["Throne", "Jade", "Gold", "Throne East", "Jade East", "Gold East", "All"]
        
        # é©—è­‰æ™‚é–“æ®µ
        for period in self.time_periods:
            if period not in valid_periods:
                raise ValueError(f"ç„¡æ•ˆçš„æ™‚é–“æ®µ: {period}ã€‚æœ‰æ•ˆé¸é …: {valid_periods}")
        
        # é©—è­‰æ®µä½
        for rank in self.ranks:
            if rank not in valid_ranks:
                raise ValueError(f"ç„¡æ•ˆçš„æ®µä½: {rank}ã€‚æœ‰æ•ˆé¸é …: {valid_ranks}")
        
        print("âœ… é…ç½®é©—è­‰é€šé")

def get_rank_display_name(rank: str) -> Dict[str, str]:
    """å–å¾—æ®µä½çš„é¡¯ç¤ºåç¨±å°æ‡‰"""
    rank_mapping = {
        "Throne": "ç‹åº§",
        "Jade": "ç‰",
        "Gold": "é‡‘",
        "Throne East": "ç‹ä¸œ",
        "Jade East": "ç‰ä¸œ", 
        "Gold East": "é‡‘ä¸œ",
        "All": "å…¨éƒ¨"
    }
    return rank_mapping.get(rank, rank)

def get_period_display_name(period: str) -> str:
    """å–å¾—æ™‚é–“æ®µçš„é¡¯ç¤ºåç¨±"""
    period_mapping = {
        "4w": "å››é€±",
        "1w": "ä¸€é€±", 
        "3d": "ä¸‰å¤©",
        "1d": "ä¸€å¤©"
    }
    return period_mapping.get(period, period)

def setup_rank_selection(driver, target_ranks: List[str]):
    """è¨­å®šæ®µä½é¸æ“‡"""
    all_available_ranks = ["Throne", "Jade", "Gold", "Throne East", "Jade East", "Gold East"]
    
    try:
        print("æ­£åœ¨é…ç½®æ®µä½é¸æ“‡...")
        
        # å¦‚æœé¸æ“‡"å…¨éƒ¨"ï¼Œç›´æ¥ä½¿ç”¨ç¶²é é è¨­ç‹€æ…‹ï¼ˆæ‰€æœ‰æ®µä½éƒ½å·²é¸ä¸­ï¼‰
        if "All" in target_ranks:
            print("é¸æ“‡å…¨éƒ¨æ®µä½ - ä½¿ç”¨ç¶²é é è¨­ç‹€æ…‹ï¼Œç„¡éœ€é»æ“Š")
            print("ç¶²é é è¨­å·²é¸ä¸­æ‰€æœ‰æ®µä½ï¼Œè·³éæ®µä½é¸æ“‡æ“ä½œ")
            return
        
        # å…ˆå–æ¶ˆé¸æ“‡æ‰€æœ‰æ®µä½
        for rank in all_available_ranks:
            try:
                # å˜—è©¦è‹±æ–‡æ¨™ç±¤
                rank_label = driver.find_element(By.XPATH, f"//span[contains(@class, 'MuiFormControlLabel-label') and text()='{rank}']")
                checkbox = rank_label.find_element(By.XPATH, "./preceding-sibling::span//input[@type='checkbox']")
                
                if checkbox.is_selected():
                    print(f"å–æ¶ˆé¸æ“‡æ®µä½: {rank}")
                    driver.execute_script("arguments[0].click();", rank_label)
                    time.sleep(0.5)
            except:
                # å˜—è©¦ä¸­æ–‡æ¨™ç±¤
                try:
                    chinese_rank = get_rank_display_name(rank)
                    rank_label = driver.find_element(By.XPATH, f"//span[contains(@class, 'MuiFormControlLabel-label') and text()='{chinese_rank}']")
                    checkbox = rank_label.find_element(By.XPATH, "./preceding-sibling::span//input[@type='checkbox']")
                    
                    if checkbox.is_selected():
                        print(f"å–æ¶ˆé¸æ“‡æ®µä½: {chinese_rank}")
                        driver.execute_script("arguments[0].click();", rank_label)
                        time.sleep(0.5)
                except:
                    continue
        
        # é¸æ“‡ç›®æ¨™æ®µä½
        for rank in target_ranks:
            try:
                # å˜—è©¦è‹±æ–‡æ¨™ç±¤
                rank_label = driver.find_element(By.XPATH, f"//span[contains(@class, 'MuiFormControlLabel-label') and text()='{rank}']")
                checkbox = rank_label.find_element(By.XPATH, "./preceding-sibling::span//input[@type='checkbox']")
                
                if not checkbox.is_selected():
                    print(f"é¸æ“‡æ®µä½: {rank}")
                    driver.execute_script("arguments[0].click();", rank_label)
                    time.sleep(0.5)
                else:
                    print(f"æ®µä½ {rank} å·²é¸ä¸­")
            except:
                # å˜—è©¦ä¸­æ–‡æ¨™ç±¤
                try:
                    chinese_rank = get_rank_display_name(rank)
                    rank_label = driver.find_element(By.XPATH, f"//span[contains(@class, 'MuiFormControlLabel-label') and text()='{chinese_rank}']")
                    checkbox = rank_label.find_element(By.XPATH, "./preceding-sibling::span//input[@type='checkbox']")
                    
                    if not checkbox.is_selected():
                        print(f"é¸æ“‡æ®µä½: {chinese_rank}")
                        driver.execute_script("arguments[0].click();", rank_label)
                        time.sleep(0.5)
                    else:
                        print(f"æ®µä½ {chinese_rank} å·²é¸ä¸­")
                except Exception as e:
                    print(f"ç„¡æ³•é¸æ“‡æ®µä½ {rank}: {e}")
        
        # ç­‰å¾…é é¢æ›´æ–°
        time.sleep(3)
        print("æ®µä½é¸æ“‡é…ç½®å®Œæˆ")
        
    except Exception as e:
        print(f"é…ç½®æ®µä½é¸æ“‡æ™‚å‡ºéŒ¯: {e}")

def get_top_players_urls(config: CrawlerConfig):
    """æ ¹æ“šé…ç½®è‡ªå‹•æŠ“å–æ’è¡Œæ¦œç©å®¶çš„URLs"""
    chrome_options = Options()
    if config.headless_mode:
        chrome_options.add_argument("--headless")
    
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-gpu")
    driver = webdriver.Chrome(options=chrome_options)
    
    all_player_urls = []
    
    try:
        # å­˜å–æ’åé é¢
        driver.get("https://amae-koromo.sapk.ch/ranking/delta")
        
        # ç­‰å¾…é é¢è¼‰å…¥å®Œæˆ
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        time.sleep(5)
        
        # å»ºç«‹æ®µä½é¡¯ç¤ºå­—ä¸²
        rank_display = ", ".join([get_rank_display_name(rank) for rank in config.ranks])
        period_display = ", ".join([get_period_display_name(period) for period in config.time_periods])
        
        print(f"æ­£åœ¨æŠ“å–æ±ªæ±ªæ¦œæ’å")
        print(f"ç›®æ¨™æ™‚é–“æ®µ: {period_display}")
        print(f"ç›®æ¨™æ®µä½: {rank_display}")
        
        # è¨­å®šæ®µä½é¸æ“‡
        setup_rank_selection(driver, config.ranks)
        
        # å„²å­˜æ®µä½é¸æ“‡é©—è­‰æˆªåœ–
        if config.save_screenshots:
            driver.save_screenshot("screenshot_rank_selection_verification.png")
            print("å·²å„²å­˜æ®µä½é¸æ“‡é©—è­‰æˆªåœ–: screenshot_rank_selection_verification.png")
        
        # è™•ç†æ¯å€‹æ™‚é–“æ®µ
        for period in config.time_periods:
            print(f"\n=== é–‹å§‹è™•ç†æ™‚é–“æ®µ: {get_period_display_name(period)} ({period}) ===")
            
            try:
                # æŸ¥æ‰¾ä¸¦é»æ“Šå°æ‡‰çš„æ™‚é–“æ®µradioæŒ‰éˆ•
                print(f"æŸ¥æ‰¾æ™‚é–“æ®µ {period} çš„radioæŒ‰éˆ•...")
                
                radio_button = driver.find_element(By.CSS_SELECTOR, f'input[type="radio"][value="{period}"]')
                print(f"æ‰¾åˆ° {period} çš„radioæŒ‰éˆ•")
                
                # é»æ“ŠradioæŒ‰éˆ•
                driver.execute_script("arguments[0].click();", radio_button)
                print(f"å·²é»æ“Š {period} æ™‚é–“æ®µ")
                
                # ç­‰å¾…é é¢æ›´æ–°
                time.sleep(5)
                
                # å„²å­˜é©—è­‰æˆªåœ–
                if config.save_screenshots:
                    rank_suffix = "_".join(config.ranks).lower()
                    screenshot_filename = f"screenshot_{period}_positive_ranking_{rank_suffix}.png"
                    driver.save_screenshot(screenshot_filename)
                    print(f"å·²å„²å­˜æˆªåœ–: {screenshot_filename}")
                
            except Exception as e:
                print(f"åˆ‡æ›åˆ°æ™‚é–“æ®µ {period} æ™‚å‡ºéŒ¯: {e}")
                continue
            
            # å–å¾—è©²æ™‚é–“æ®µçš„ç©å®¶é€£çµ
            period_player_urls = extract_positive_ranking_players(driver, period, config)
            all_player_urls.extend(period_player_urls)
            
            print(f"æ™‚é–“æ®µ {period} å–å¾—åˆ° {len(period_player_urls)} å€‹ç©å®¶URL")
        
        # å»é‡è™•ç†
        unique_player_urls = []
        seen_players = set()
        
        for url in all_player_urls:
            player_id_match = re.search(r'/player/(\d+)', url)
            if player_id_match:
                player_id = player_id_match.group(1)
                if player_id not in seen_players:
                    seen_players.add(player_id)
                    unique_player_urls.append(url)
        
        print(f"å…±å–å¾—åˆ° {len(unique_player_urls)} å€‹ä¸é‡è¤‡çš„ç©å®¶URLsï¼ˆ/12æ¨¡å¼ï¼‰")
        
        if config.save_screenshots:
            print(f"\nğŸ“¸ é©—è­‰æˆªåœ–å·²å„²å­˜:")
            print(f"  - screenshot_rank_selection_verification.png (æ®µä½é¸æ“‡é©—è­‰)")
            for period in config.time_periods:
                rank_suffix = "_".join(config.ranks).lower()
                print(f"  - screenshot_{period}_positive_ranking_{rank_suffix}.png ({get_period_display_name(period)})")
        
        return unique_player_urls
        
    except Exception as e:
        print(f"æŠ“å–æ’åæ™‚å‡ºéŒ¯: {e}")
        return []
    
    finally:
        driver.quit()

def extract_positive_ranking_players(driver, period, config: CrawlerConfig):
    """å¾Positive rankingåˆ—ä¸­æå–ç©å®¶é€£çµ"""
    player_urls = []
    
    try:
        print(f"é–‹å§‹æŸ¥æ‰¾æ™‚é–“æ®µ {period} çš„Positive rankingåˆ—ä¸­çš„ç©å®¶é€£çµ...")
        
        # æ–¹æ³•1ï¼šå˜—è©¦æŸ¥æ‰¾Positive rankingåˆ—ä¸­çš„ç©å®¶é€£çµ
        player_links_in_positive = []
        
        try:
            positive_heading = driver.find_element(By.XPATH, "//*[contains(text(), 'Positive ranking')]")
            print("æ‰¾åˆ°Positive rankingæ¨™é¡Œ")
            
            positive_container = positive_heading.find_element(By.XPATH, "./following-sibling::*[1] | ./parent::*/following-sibling::*[1]")
            
            container_links = positive_container.find_elements(By.CSS_SELECTOR, "a[href*='/player/']")
            player_links_in_positive.extend(container_links)
            print(f"åœ¨Positive rankingå®¹å™¨ä¸­æ‰¾åˆ° {len(container_links)} å€‹ç©å®¶é€£çµ")
            
        except Exception as e:
            print(f"æ–¹æ³•1å¤±æ•—: {e}")
        
        # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±æ•—ï¼Œå˜—è©¦é€šéé é¢ä½ˆå±€å®šä½
        if not player_links_in_positive:
            try:
                print("å˜—è©¦æ–¹æ³•2ï¼šé€šéé é¢ä½ˆå±€å®šä½Positive ranking...")
                
                all_player_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='/player/']")
                
                for link in all_player_links:
                    try:
                        location = link.location
                        size = driver.get_window_size()
                        
                        if size['width'] * 0.33 < location['x'] < size['width'] * 0.66:
                            player_links_in_positive.append(link)
                    except:
                        continue
                
                print(f"æ–¹æ³•2æ‰¾åˆ° {len(player_links_in_positive)} å€‹å¯èƒ½çš„Positive rankingé€£çµ")
                
            except Exception as e:
                print(f"æ–¹æ³•2ä¹Ÿå¤±æ•—: {e}")
        
        # æ–¹æ³•3ï¼šå¦‚æœå‰å…©ç¨®æ–¹æ³•éƒ½å¤±æ•—ï¼Œå–å¾—æ‰€æœ‰ç©å®¶é€£çµä¸¦éæ¿¾
        if not player_links_in_positive:
            print("å˜—è©¦æ–¹æ³•3ï¼šå–å¾—æ‰€æœ‰ç©å®¶é€£çµ...")
            all_links = driver.find_elements(By.TAG_NAME, "a")
            for link in all_links:
                href = link.get_attribute("href")
                if href and "/player/" in href:
                    player_links_in_positive.append(link)
            
            print(f"æ–¹æ³•3æ‰¾åˆ° {len(player_links_in_positive)} å€‹ç©å®¶é€£çµ")
            if len(player_links_in_positive) >= 60:
                start_idx = len(player_links_in_positive) // 3
                end_idx = start_idx + config.max_players_per_period
                player_links_in_positive = player_links_in_positive[start_idx:end_idx]
                print(f"éæ¿¾å¾Œä¿ç•™ {len(player_links_in_positive)} å€‹Positive rankingé€£çµ")
        
        # æå–æŒ‡å®šæ•¸é‡çš„ä¸é‡è¤‡ç©å®¶URL
        seen_players = set()
        for link in player_links_in_positive:
            href = link.get_attribute("href") if hasattr(link, 'get_attribute') else getattr(link, 'href', None)
            if href and "/player/" in href:
                player_id_match = re.search(r'/player/(\d+)', href)
                if player_id_match:
                    player_id = player_id_match.group(1)
                    if player_id not in seen_players:
                        seen_players.add(player_id)
                        base_url = f"https://amae-koromo.sapk.ch/player/{player_id}/12"
                        url = f"{base_url}?limit={config.paipu_limit}"
                        player_urls.append(url)
                        print(f"æ·»åŠ ç©å®¶URL ({period}): {url}")
                        
                        if len(player_urls) >= config.max_players_per_period:
                            break
        
    except Exception as e:
        print(f"æå–æ™‚é–“æ®µ {period} çš„ç©å®¶é€£çµæ™‚å‡ºéŒ¯: {e}")
    
    return player_urls

def process_player(url, processed_paipu_ids, player_counts, config: CrawlerConfig):
    """è™•ç†å–®å€‹ç©å®¶çš„ç‰Œè­œæŠ“å–"""
    chrome_options = Options()
    if config.headless_mode:
        chrome_options.add_argument("--headless")
    driver = webdriver.Chrome(options=chrome_options)
    
    try:
        driver.get(url)

        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        time.sleep(2)

        while True:
            paipu_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='paipu=']")
            
            if not paipu_links:
                paipu_links = driver.find_elements(By.XPATH, "//a[contains(@href, 'paipu=')]")

            new_paipu_found = False
            for link in paipu_links:
                href = link.get_attribute("href")
                if href and "paipu=" in href:
                    paipu_id = href.split("paipu=")[1].split("_")[0]
                    
                    if paipu_id not in processed_paipu_ids:
                        processed_paipu_ids.append(paipu_id)
                        player_counts[url] += 1
                        print(f"å·²å¯«å…¥æ–°çš„ç‰Œè­œ ({url}):", paipu_id)
                        new_paipu_found = True
            
            driver.execute_script("window.scrollBy(0, 500);")
            time.sleep(0.3)

            if driver.execute_script("return window.innerHeight + window.scrollY + 10 >= document.body.offsetHeight"):
                break
                
            if not new_paipu_found:
                time.sleep(1)

        print(f"ç©å®¶ {url} æ”¶é›†åˆ° {player_counts[url]} å€‹ç‰Œè­œID")
        
    except Exception as e:
        print(f"è™•ç†ç©å®¶ {url} æ™‚å‡ºéŒ¯: {e}")
    finally:
        driver.quit()

class PaipuSpider(scrapy.Spider):
    name = "paipu_spider"

    def __init__(self, config_path: str = "crawler_config.json", use_manual_urls: bool = False):
        # è¼‰å…¥é…ç½®
        self.config = CrawlerConfig.from_json(config_path)
        self.config.validate()
        
        self.manager = multiprocessing.Manager()
        self.processed_paipu_ids = self.manager.list()
        
        # æ±ºå®šä½¿ç”¨è‡ªå‹•åŒ–é‚„æ˜¯æ‰‹å‹•é…ç½®
        if use_manual_urls or hasattr(self, 'manual_player_urls'):
            print("ğŸ”§ ä½¿ç”¨ Legacy Manual æ¨¡å¼...")
            print("å¾ç¨‹å¼ç¢¼ä¸­è®€å–æ‰‹å‹•è¨­å®šçš„ç©å®¶URLs")
            
            # Legacy Manual URLs (åœ¨é€™è£¡æ‰‹å‹•æ·»åŠ ç©å®¶URLs)
            manual_urls = getattr(self, 'manual_player_urls', [
                # åœ¨é€™è£¡æ·»åŠ æ‰‹å‹•ç©å®¶URLsï¼Œä¾‹å¦‚ï¼š
                # "https://amae-koromo.sapk.ch/player/123456/12?limit=9999",
                # "https://amae-koromo.sapk.ch/player/789012/12?limit=9999",
            ])
            
            if manual_urls:
                self.player_urls = manual_urls
                print(f"å·²è¼‰å…¥ {len(self.player_urls)} å€‹æ‰‹å‹•è¨­å®šçš„ç©å®¶URLs")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°æ‰‹å‹•è¨­å®šçš„ç©å®¶URLsï¼Œåˆ‡æ›åˆ°è‡ªå‹•åŒ–æ¨¡å¼")
                use_manual_urls = False
        
        if not use_manual_urls:
            print("ğŸš€ ä½¿ç”¨è‡ªå‹•åŒ–é…ç½®æ¨¡å¼...")
            print(f"é…ç½®æ‘˜è¦:")
            print(f"  æ™‚é–“æ®µ: {[get_period_display_name(p) for p in self.config.time_periods]}")
            print(f"  æ®µä½: {[get_rank_display_name(r) for r in self.config.ranks]}")
            print(f"  æ¯å€‹æ™‚é–“æ®µæœ€å¤šç©å®¶æ•¸: {self.config.max_players_per_period}")
            print(f"  ç‰Œè­œé™åˆ¶: {self.config.paipu_limit}")
            
            self.player_urls = get_top_players_urls(self.config)
        
        self.player_counts = self.manager.dict({url: 0 for url in self.player_urls})

        # è®€å–å·²æœ‰çš„ç‰Œè­œID
        try:
            with open(self.config.output_filename, "r") as file:
                for line in file:
                    paipu_id = line.strip()
                    if paipu_id:
                        self.processed_paipu_ids.append(paipu_id)
            print(f"å·²è¼‰å…¥ {len(self.processed_paipu_ids)} å€‹å·²è™•ç†çš„ç‰Œè­œID")
        except FileNotFoundError:
            print(f"æœªæ‰¾åˆ°{self.config.output_filename}æª”æ¡ˆï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆ")

    def start_requests(self):
        yield scrapy.Request(url="https://amae-koromo.sapk.ch", callback=self.start_crawling)

    def start_crawling(self, response):
        print(f"é–‹å§‹è™•ç† {len(self.player_urls)} å€‹ç©å®¶...")
        
        processes = []
        for url in self.player_urls:
            process = multiprocessing.Process(target=process_player, args=(url, self.processed_paipu_ids, self.player_counts, self.config))
            processes.append(process)
            process.start()

        for process in processes:
            process.join()

        self.spider_closed(None)

    def spider_closed(self, reason):
        print(f"å…±æ”¶é›†åˆ° {len(self.processed_paipu_ids)} å€‹ä¸é‡è¤‡çš„ç‰Œè­œID")
        print("å„ç©å®¶æ”¶é›†åˆ°çš„ç‰Œè­œIDæ•¸é‡:")
        
        total_paipu = 0
        for url in self.player_urls:
            count = self.player_counts[url]
            total_paipu += count
            print(f"{url}: {count}")
        
        print(f"\nç¸½è¨ˆæ”¶é›†ç‰Œè­œæ•¸é‡: {total_paipu}")
        
        # é¡¯ç¤ºé…ç½®æ‘˜è¦
        print(f"\nğŸ“‹ ä½¿ç”¨çš„é…ç½®:")
        print(f"  æ™‚é–“æ®µ: {', '.join([get_period_display_name(p) for p in self.config.time_periods])}")
        print(f"  æ®µä½: {', '.join([get_rank_display_name(r) for r in self.config.ranks])}")
        
        if self.config.save_screenshots:
            print(f"\nğŸ“¸ é©—è­‰æˆªåœ–å·²å„²å­˜:")
            print(f"  - screenshot_rank_selection_verification.png (æ®µä½é¸æ“‡é©—è­‰)")
            for period in self.config.time_periods:
                rank_suffix = "_".join(self.config.ranks).lower()
                print(f"  - screenshot_{period}_positive_ranking_{rank_suffix}.png ({get_period_display_name(period)})")

        with ThreadPoolExecutor() as executor:
            executor.submit(self.write_to_file)

    def write_to_file(self):
        with open(self.config.output_filename, "w") as file:
            for paipu_id in self.processed_paipu_ids:
                file.write(paipu_id + "\n")
        print(f"ç‰Œè­œIDå·²å„²å­˜åˆ° {self.config.output_filename}")

# å»ºç«‹é è¨­é…ç½®æª”æ¡ˆçš„å‡½æ•¸
def create_default_config():
    """å»ºç«‹é è¨­é…ç½®æª”æ¡ˆ"""
    config = CrawlerConfig.get_default_config()
    config.save_to_json("crawler_config.json")
    print("å·²å»ºç«‹é è¨­é…ç½®æª”æ¡ˆ: crawler_config.json")
    return config

# ==========================================
# Legacy Manual ä½¿ç”¨ç¯„ä¾‹
# ==========================================

class ManualPaipuSpider(PaipuSpider):
    """æ‰‹å‹•é…ç½®ç©å®¶URLsçš„Spideré¡åˆ¥"""
    
    def __init__(self):
        # æ‰‹å‹•è¨­å®šç©å®¶URLsï¼ˆLegacyæ–¹å¼ï¼‰
        self.manual_player_urls = [
            "https://amae-koromo.sapk.ch/player/123456789/12?limit=9999",
            "https://amae-koromo.sapk.ch/player/987654321/12?limit=9999",
            "https://amae-koromo.sapk.ch/player/555666777/12?limit=9999",
            # åœ¨é€™è£¡æ·»åŠ æ›´å¤šç©å®¶URLs...
        ]
        
        # å‘¼å«çˆ¶é¡åˆå§‹åŒ–ï¼Œå•Ÿç”¨æ‰‹å‹•æ¨¡å¼
        super().__init__(use_manual_urls=True)

# ==========================================
# ä½¿ç”¨èªªæ˜å’ŒåŸ·è¡Œæ–¹å¼
# ==========================================

if __name__ == "__main__":
    # æ–¹å¼1ï¼šè‡ªå‹•åŒ–é…ç½®æ¨¡å¼ï¼ˆæ¨è–¦ï¼‰
    # ä½¿ç”¨ crawler_config.json é…ç½®æª”æ¡ˆ
    # åŸ·è¡Œå‘½ä»¤ï¼šscrapy crawl paipu_spider
    
    # æ–¹å¼2ï¼šLegacy Manual æ¨¡å¼
    # 1. ä¿®æ”¹ä¸Šé¢çš„ manual_player_urls åˆ—è¡¨
    # 2. è¨»å†Šæ–°çš„spiderï¼šåœ¨ settings.py æˆ–ç›´æ¥åŸ·è¡Œ
    # 3. åŸ·è¡Œå‘½ä»¤ï¼šscrapy crawl manual_paipu_spider
    
    # æ–¹å¼3ï¼šæ··åˆæ¨¡å¼ - åœ¨ç¾æœ‰ç¨‹å¼ä¸­ç›´æ¥è¨­å®š
    # spider = PaipuSpider(use_manual_urls=True)
    # spider.manual_player_urls = ["URL1", "URL2", ...]
    
    # å¦‚æœé…ç½®æª”æ¡ˆä¸å­˜åœ¨ï¼Œå»ºç«‹é è¨­é…ç½®
    import os
    if not os.path.exists("crawler_config.json"):
        create_default_config()
        print("è«‹ç·¨è¼¯ crawler_config.json ä¾†è‡ªè¨‚æ‚¨çš„æŠ“å–è¨­å®š")
    else:
        print("ç™¼ç¾ç¾æœ‰é…ç½®æª”æ¡ˆ: crawler_config.json")